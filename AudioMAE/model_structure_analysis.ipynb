{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from audio_mae import AudioMaskedAutoencoderViT\n",
    "from functools import partial\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder 조사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. patch embedding - 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. position embedding 과정 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 768])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "pos_embedding = nn.Parameter(torch.zeros(1, 512 + 1, 768), requires_grad=False)[:, 1:, :]\n",
    "print(pos_embedding.shape)\n",
    "print(pos_embedding[:, :1, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "cls_token = nn.Parameter(torch.zeros(1, 1, 768)) \n",
    "print(cls_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([2, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "cls_token = cls_token + pos_embedding[:, :1, :]\n",
    "print(cls_token.shape)\n",
    "cls_tokens = cls_token.expand(2, -1, -1)\n",
    "print(cls_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 103, 768])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones([2, 102, 768])\n",
    "x = torch.cat((cls_tokens, x), dim=1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. random masking 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ratio = 0.8\n",
    "x = torch.rand([1, 512, 768]) # batch, patch 개수, embedding dimmension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, L, D = x.shape  # batch, length, dim [2, 512, 768]\n",
    "len_keep = int(L * (1 - mask_ratio)) # 512 * (1 - 0.8 ) = 102\n",
    "\n",
    "noise = torch.rand(N, L, device=x.device)  # noise in [0, 1], noise shape : [N, L], [batch, patch 개수], [2, 512]\n",
    "\n",
    "# sort noise for each sample\n",
    "ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove # torch.Size([2, 512])\n",
    "ids_restore = torch.argsort(ids_shuffle, dim=1) # HHJ : ids_restore가 tensor element의 순서를 나타냄\n",
    "\n",
    "# keep the first subset\n",
    "ids_keep = ids_shuffle[:, :len_keep]\n",
    "x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D)) # ids_keep.unsqueeze(-1) : torch.Size([2, 102, 1]) ids_keep.unsqueeze(-1).repeat(1, 1, D) : torch.Size([2, 102, 768])\n",
    "# x_masked : torch.Size([2, 102, 768])\n",
    "\n",
    "# generate the binary mask: 0 is keep, 1 is remove\n",
    "mask = torch.ones([N, L], device=x.device)\n",
    "mask[:, :len_keep] = 0\n",
    "# unshuffle to get the binary mask\n",
    "mask = torch.gather(mask, dim=1, index=ids_restore) # [N, L], [2, 512]\n",
    "\n",
    "# return x_masked, mask, ids_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4548, 0.2263, 0.0600],\n",
       "         [0.0222, 0.2445, 0.2514],\n",
       "         [0.0398, 0.6840, 0.2381],\n",
       "         [0.0882, 0.4236, 0.3248]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_ratio = 0.25\n",
    "x = torch.rand([1, 4, 3]) # batch, patch 개수, embedding dimmension\n",
    "x = torch.tensor([[[0.4548, 0.2263, 0.0600],\n",
    "         [0.0222, 0.2445, 0.2514],\n",
    "         [0.0398, 0.6840, 0.2381],\n",
    "         [0.0882, 0.4236, 0.3248]]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise : tensor([[0.2119, 0.0595, 0.3355, 0.0211]])\n",
      "ids_shuffle : tensor([[3, 1, 0, 2]])\n",
      "ids_restore : tensor([[2, 1, 3, 0]])\n",
      "ids_keep : tensor([[3, 1, 0]])\n",
      "index = tensor([[[3, 3, 3],\n",
      "         [1, 1, 1],\n",
      "         [0, 0, 0]]])\n",
      "x_masked : tensor([[[0.0882, 0.4236, 0.3248],\n",
      "         [0.0222, 0.2445, 0.2514],\n",
      "         [0.4548, 0.2263, 0.0600]]])\n",
      "tensor([[1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 1.]])\n",
      "mask : tensor([[0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# print(x)\n",
    "N, L, D = x.shape  # torch.Size([2, 4, 3])\n",
    "len_keep = int(L * (1 - mask_ratio)) # 3\n",
    "\n",
    "noise = torch.tensor([[0.2119, 0.0595, 0.3355, 0.0211]]) \n",
    "print(f\"Noise : {noise}\")\n",
    "\n",
    "ids_shuffle = torch.argsort(noise, dim=1) \n",
    "ids_restore = torch.argsort(ids_shuffle, dim=1) \n",
    "print(f\"ids_shuffle : {ids_shuffle}\")\n",
    "print(f\"ids_restore : {ids_restore}\")\n",
    "\n",
    "# keep the first subset\n",
    "ids_keep = ids_shuffle[:, :len_keep]\n",
    "print(f\"ids_keep : {ids_keep}\")\n",
    "print(f\"index = {ids_keep.unsqueeze(-1).repeat(1, 1, D)}\")\n",
    "x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D)) \n",
    "print(f\"x_masked : {x_masked}\")\n",
    "\n",
    "mask = torch.ones([N, L], device=x.device)\n",
    "print(mask)\n",
    "mask[:, :len_keep] = 0\n",
    "print(mask)\n",
    "# unshuffle to get the binary mask\n",
    "mask = torch.gather(mask, dim=1, index=ids_restore) # [N, L], [2, 512]\n",
    "print(f\"mask : {mask}\")\n",
    "\n",
    "# return x_masked, mask, ids_restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decoder 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : torch.Size([2, 1, 1024, 128]), before patch_embed\n",
      "x : torch.Size([2, 512, 768]), after patch_embed\n",
      "x : torch.Size([2, 512, 768]), before random masking\n",
      "x : torch.Size([2, 102, 768]), after random masking\n",
      "cls : torch.Size([2, 1, 768])\n",
      "x : torch.Size([2, 103, 768]), before encoder\n",
      "x : torch.Size([2, 103, 768])\n",
      "latent: torch.Size([2, 103, 768]) torch.Size([2, 512]) torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "audio_mels = torch.ones([2, 1, 1024, 128])\n",
    "\n",
    "# Paper recommended archs\n",
    "model  = AudioMaskedAutoencoderViT(\n",
    "        num_mels=128, mel_len=1024, in_chans=1,\n",
    "        patch_size=16, embed_dim=768, encoder_depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=16, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "        \n",
    "latent, msk,  ids_restore = model.forward_encoder(audio_mels, mask_ratio = 0.8)\n",
    "print(\"latent:\", latent.shape ,msk.shape, ids_restore.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : torch.Size([2, 103, 768])\n",
      "x after decoder_embed : torch.Size([2, 102, 512])\n",
      "mask_tokens : torch.Size([2, 410, 512])\n",
      "x_cat : torch.Size([2, 512, 512])\n",
      "x : torch.Size([2, 512, 512])\n",
      "x before pos embed: torch.Size([2, 512, 512])\n",
      "x after pos embed: torch.Size([2, 512, 512])\n",
      "x after reshape: torch.Size([2, 64, 8, 512])\n",
      "x after transformer block: torch.Size([2, 64, 8, 512])\n",
      "x after rerrange: torch.Size([2, 512, 512])\n",
      "x after decoder_norm: torch.Size([2, 512, 512])\n",
      "x predictor projection: torch.Size([2, 512, 256])\n"
     ]
    }
   ],
   "source": [
    "x = latent\n",
    "ids_restore = ids_restore\n",
    "\n",
    "print(f\"x : {x.shape}\") # x : torch.Size([2, 103, 768])\n",
    "x = model.decoder_embed(x[:, 1:, :]) # nn.Linear\n",
    "print(f\"x after decoder_embed : {x.shape}\") # torch.Size([2, 102, 512])\n",
    "\n",
    "# append mask tokens to sequence\n",
    "\n",
    "mask_tokens = model.mask_token.repeat(x.shape[0], ids_restore.shape[1] - x.shape[1], 1)\n",
    "print(f\"mask_tokens : {mask_tokens.shape}\") # torch.Size([2, 410, 512])\n",
    "x_ = torch.cat([x, mask_tokens], dim=1)  # no cls token\n",
    "print(f\"x_cat : {x_.shape}\") # torch.Size([2, 512, 512])\n",
    "x = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "print(f\"x : {x.shape}\") # torch.Size([2, 512, 512])\n",
    "\n",
    "b, l, c = x.shape\n",
    "\n",
    "assert l == model.grid_h * model.grid_w, \"input feature has wrong size\"\n",
    "\n",
    "# add pos embed\n",
    "print(f\"x before pos embed: {x.shape}\") # torch.Size([2, 512, 512])\n",
    "x = x + model.decoder_pos_embed\n",
    "print(f\"x after pos embed: {x.shape}\") # torch.Size([2, 512, 512])\n",
    "x = x.view(b, model.grid_h, model.grid_w, c)\n",
    "print(f\"x after reshape: {x.shape}\") # torch.Size([2, 64, 8, 512])\n",
    "# apply Transformer blocks\n",
    "for blk in model.decoder_blocks:\n",
    "    x = blk(x)\n",
    "print(f\"x after transformer block: {x.shape}\") # torch.Size([2, 64, 8, 512])\n",
    "x = rearrange(x, 'b h w c -> b (h w) c')\n",
    "print(f\"x after rerrange: {x.shape}\") # torch.Size([2, 512, 512])\n",
    "x = model.decoder_norm(x)\n",
    "print(f\"x after decoder_norm: {x.shape}\") # torch.Size([2, 512, 512])\n",
    "# predictor projection\n",
    "x = model.decoder_pred(x)\n",
    "print(f\"x predictor projection: {x.shape}\") # torch.Size([2, 512, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4189, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "imgs, pred, mask = audio_mels, x, msk\n",
    "\n",
    "loss =  model.forward_loss(audio_mels, pred, mask)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs : torch.Size([2, 1, 1024, 128])\n",
      "after patchfy : torch.Size([2, 512, 256])\n",
      "pred : torch.Size([2, 512, 256])\n"
     ]
    }
   ],
   "source": [
    "print(f\"imgs : {imgs.shape}\")\n",
    "\n",
    "target = model.patchify(imgs)\n",
    "\n",
    "print(f\"after patchfy : {target.shape}\")\n",
    "print(f\"pred : {pred.shape}\")\n",
    "if model.norm_pix_loss:\n",
    "    print(\"1\")\n",
    "    mean = target.mean(dim=-1, keepdim=True)\n",
    "    var = target.var(dim=-1, keepdim=True)\n",
    "    target = (target - mean) / (var + 1.e-6) ** .5\n",
    "\n",
    "loss = (pred - target) ** 2\n",
    "loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpatch = model.unpatchify(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1024, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpatch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 구조 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_mels = torch.ones([2, 1, 1024, 128])\n",
    "\n",
    "# Paper recommended archs\n",
    "model  = AudioMaskedAutoencoderViT(\n",
    "        num_mels=128, mel_len=1024, in_chans=1,\n",
    "        patch_size=16, embed_dim=768, encoder_depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=16, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, pred, mask = model(audio_mels)\n",
    "print(loss, pred.shape, mask.shape) # mask_ratio = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 의문 : torch.Size([2, 512, 256]) 이 사이즈는 어떻게 나왔나?\n",
    "- forward_encoder에서 mask의 값이긴 함\n",
    "- 아래 cell에서 확인할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent, msk,  ids_restore = model.forward_encoder(audio_mels, mask_ratio = 0.8)\n",
    "print(\"latent:\", latent.shape ,msk.shape, ids_restore.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.forward_decoder(latent, ids_restore) \n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss =  model.forward_loss(audio_mels, pred, mask)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transform import MelSpectrogram_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load('./N6e5C5sXdBI_0.000_10.000.wav', sr = 16000)\n",
    "print(len(audio), sr, len(audio) / sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "\n",
    "sr = 16000\n",
    "\n",
    "spectrogram = T.MelSpectrogram(sample_rate= sr,\n",
    "                            hop_length = int(sr * 0.01),\n",
    "                            n_fft = 512,\n",
    "                            n_mels = 128,\n",
    "                            window_fn = torch.hann_window,f_max=8000\n",
    "                            ) # 1 : energy, 2 : power\n",
    "\n",
    "spectrogram_a = T.Spectrogram(hop_length = int(sr * 0.01),\n",
    "                            n_fft = 512,\n",
    "                            window_fn = torch.hann_window) # 1 : energy, 2 : power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_a(torch.ones(160000)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sr * 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = int(sr * 0.025)\n",
    "hop_len = int(sr * 0.01)\n",
    "print(win_len, hop_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = MelSpectrogram_transform(sample_rate = sr, hop_length=hop_len, win_length=win_len, n_fft = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = mel_spectrogram(torch.Tensor(audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = mel.transpose(0, 1)\n",
    "print(mel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = mel.unsqueeze(dim = 0)\n",
    "mel = mel.unsqueeze(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent, mask, ids_restore = model.forward_encoder(mel, mask_ratio = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('creativity')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3384a607f74b4fca5e031a21f0c51d2ef3fcb4df6d509bae4f530b379b99357e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
